<!DOCTYPE html>
<html>

<head>
	<title>Shaders - Anton's OpenGL 4 Tutorials</title>
	<link rel="stylesheet" type="text/css" href="../greysans.css">
</head>

<body>

	<p><a href="index.html">[index]</a></p>

	<h1>OpenGL 4 Shaders</h1>
	<h4>Anton Gerdelan. Last edited: 19 Jan 2025</h4>

	<p>
		Shaders tell OpenGL <i>how</i> to draw, and we have a lot of creative opportunity to do interesting effects.
		We'll cover the <i>what</i> to draw, the geometry, in more detail in the following article.
	</p>

	<h2>Overview</h2>

	<p>
		Shaders are mini-programs that define a <i>style</i> of rendering. They are compiled, by a special shader
		compiler,
		and run on the specialised GPU (graphics processing unit). The GPU is built to do lots of floating point
		operations in parallel (at once).
		Many more than your CPU (central processing unit) can.
		You should look up the statistics for your particular GPU's float32 operations per second (flop/s),
		and compare that to your particular CPU model's flop/s to get a rough idea.
		Many of the tasks in computer graphics fall into the category of "<i>embarrassingly parallel</i>" problems:
	</p>

	<ul>
		<li>Setting the position in our scene for each vertex point in a 3D shape. We do this with a vertex shader.</li>
		<li>Setting the colour of each pixel-sized piece of shapes we render. We do this in a fragment shader.</li>
		<li>Casting and reflecting each ray of light in a <i>ray-traced</i> scene. It's possible to do this, for
			example, in a fragment shader.</li>
		<li>Calculating the physics of individual particles in e.g. a fluid simulation. It's possible to do this using
			another type of shader called a <i>compute shader</i>.</li>
		<li>And many other tasks.</li>
	</ul>

	<p>
		It's also convenient to make use of lots of floating point numbers in these calculations. We can use them for
		positions in 3D,
		for the values of colours, for timers, and lots of in-between variables we make up ourselves like the intensity
		of lights in our scenes.
		You can see how the GPU hardware is going to be really useful here. Because the GPU can process many floating
		point instructions at once, you can guess why our shaders
		are designed to operate on only one item { vertex, fragment, etc. }, and not loop over all of them. We're
		trying to make the most use of the GPU's highly parallel architecture,
		and run as many things at once as we can. Thus, each vertex we position will invoke another vertex shader,
		each fragment will invoke another of our fragment shaders, and the GPU can schedule as much as possible to run
		in parallel.
	</p>

	<div class="hint">
		We don't need to know the details of GPU hardware architecture to start writing shaders
		for OpenGL, but as an aside, here's an overview. Note that terms for each part differ by manufacturer.
		A GPU has a block of processing cores. These aren't really equivalent to your CPU's cores.
		Each core has a hierarchical organisation, comprising groups of <i>shader cores</i>,
		and possibly also specialised cores for AI and ray tracing. The shader cores, in turn, contain groups of
		<i>threads</i>
		that execute the instructions from your shaders. Each thread group is called a
		<i>warp</i>, or <i>wavefront</i>.
		There are usually 32 or 64 threads in a warp. A warp runs the same instruction over all 32 threads,
		and each can use different data. This is called SIMD (single instruction, multiple data).
	</div>

	<div class="centre">
		<img src="images/shaders_2025_redo_512.png" /><br />
		<small><i>Common drawing operations such as transforming each vertex and
				colouring each fragment can be done independently. We write a shader (mini program) to
				compute these operations, and the GPU's highly parallel architecture will
				attempt to compute them all concurrently.</i></small>
	</div>

	<p>
		Shaders are a way of re-programming the graphics pipeline. If we wanted to use a different
		colouring method for the cube in the image, or have an animated, spinning cube, we could tell OpenGL
		to switch to using a different shader program. The rendering process has several
		distinct stages of transforming a 3D object in a final 2D image. We call this staged
		process the <b>graphics pipeline</b>.
		All of the stages of the graphics pipeline that happen on the GPU are called the
		<b>[programmable] hardware pipeline</b>. Older OpenGL APIs had pre-canned functions like
		<tt>glLight()</tt> for driving the rendering model. We call this the <b>fixed-function
			pipeline</b> ("fixed" because it's not re-programmable). These functions no longer exist,
		and we have to write the lighting equations ourselves in shaders. In OpenGL 4 we can
		write a shader to control many different stages of the graphics pipeline:
	</p>

	<div class="centre">
		<img src="images/hwpipe2.png" width="512px" />
	</div>

	<p>
		A complete shader program comprises a set of separate shader
		(mini-programs) - one to control each stage. Each mini-program - called a <tt><b>shader</b></tt>
		by OpenGL - is compiled, and the whole set are linked together to form the executable
		shader program - called a <tt>program</tt> by OpenGL. Yes, that's confusing! If you look at the Quick
		Reference Card (or further down the page) you can see that the API differentiates
		functions into <tt>glShader</tt> and <tt>glProgram</tt>.
	</p>


	<div class="hint">
		Note that there is a distinction between a shader, indicated by white boxes in the pipeline image, above, which
		is a mini-program for just
		one stage in the hardware pipeline, and a shader program, which is a GPU program
		that comprises several shaders that have been linked together.
	</div>

	<p>
		Each individual shader has a different job. At minimum, we usually have 1
		vertex shader and 1 fragment shader per shader program, but OpenGL 4 allows us to use
		some optional shaders too.
	</p>

	<h2>Shader Parallelism</h2>

	<p>
		Shader programs run on the GPU, and are highly parallelised. Each vertex shader only
		transforms 1 vertex. If we have a mesh of 2000 vertices, then 2000 vertex shaders will
		be launched when we draw it. Because we can compute each one separately, we can also run them
		all in parallel. Depending on the number of processors on the GPU, you might
		be able to compute all of your mesh's vertex shaders simultaneously.
	</p>

	<table>
		<caption><b>Comparison of Selected OpenGL4-Capable GPUs</b></caption>
		<tr>
			<th>Release Date</th>
			<th>GPU type</th>
			<th>GPU cores / shading units</th>
		</tr>
		<tr>
			<td>2017</td>
			<td>GeForce GTX 1080 Ti</td>
			<td>3584</td>
		</tr>
		<tr>
			<td>2022</td>
			<td>Radeon RX 7900 XTX</td>
			<td>6144</td>
		</tr>
		<tr>
			<td>2022</td>
			<td>GeForce RTX 4090</td>
			<td>16384</td>
		</tr>
	</table>

	<p>
		Because there is a lot of variation in user GPU hardware, we can only make very general
		assumptions about the ideal number of vertices or facets each mesh should have for
		best performance. Because we only draw one mesh at a time, keeping the number of
		<u>separate meshes drawn</u> per-scene to a low-ish level is often beneficial
		(reducing the <b>batch count</b> <i>per</i> rendered frame) - the idea is to keep as
		many of the processors in use at once as possible.
	</p>


	<div class="hint">
		GPUs don't use <i>branch prediction</i> optimisations like a CPU does. Things like if-statements introduce
		branching.
		In SIMD architectures the GPU might have to execute <i>both</i> paths of an if-else condition, one after the
		other in sequence.
		This could also leave some threads idle. That would essentially make your shader take longer to render, and
		might mean your
		rendering performs worse. The advice would generally be to use branching sparingly in shaders.
		GPUs since about 2016 use newer SIMT (single instruction multiple thread) architecture which can improve
		branching performance.
		So, the advice might be, don't be scared to use branching, but if you want to improve performance, try removing
		some, and check the
		difference on a couple of different machines.</div>

	<h2>Difference Between Fragments and Pixels</h2>

	<p>
		A <b>pixel</b> is a "picture element". In OpenGL lingo, pixels are the
		elements that make up the final 2D image that it draws inside a window on your
		display. A <b>fragment</b> is a pixel-sized area of a surface. A fragment
		shader determines the colour of each one. Sometimes surfaces overlap - we then
		have more than 1 fragment for 1 pixel. <b>All of the fragments are drawn, even
			the hidden ones</b>.
	</p>

	<p>
		Each fragment is written into the framebuffer image that will be displayed as
		the final pixels. If <i>depth testing</i> is enabled it will paint the
		front-most fragments on top of the further-away fragments. In this case, when
		a farther-away fragment is drawn after a closer fragment, then the GPU is
		clever enough to skip drawing it, but it's actually quite tricky to organise
		the scene to take advantage of this, so we'll often end up executing many redundant fragment shader invocations.
	</p>

	<h2>Shader Language</h2>

	<p>
		OpenGL 4 shaders are written in OpenGL Shader Language version 4.00.9. The GLSL language
		from OpenGL versions 3 to 4 is almost identical, so we can port between versions
		without changing the code. OpenGL version 3.2 added a new type of shader: geometry
		shaders, and version 4.0 added tessellation control and tessellation evaluation shaders.
		These, of course, can not be rolled back to earlier versions. The first line in a GLSL
		shader should start with the simplified version tag:
	</p>

	<p><tt>#version 410 core</tt></p>

	<p>
		The different version tags are:
	</p>

	<table>
		<caption><b>Version Tags for OpenGL and GLSL Versions</b></caption>
		<tr>
			<th>OpenGL Version</th>
			<th>GLSL Version</th>
			<th><tt>#version</tt> tag</th>
		</tr>
		<tr>
			<td>1.2</td>
			<td>none</td>
			<td>none</td>
		</tr>
		<tr>
			<td>2.0</td>
			<td>1.10.59</td>
			<td>110</td>
		</tr>
		<tr>
			<td>2.1</td>
			<td>1.20.8</td>
			<td>120</td>
		</tr>
		<tr>
			<td>3.0</td>
			<td>1.30.10</td>
			<td>130</td>
		</tr>
		<tr>
			<td>3.1</td>
			<td>1.40.08</td>
			<td>140</td>
		</tr>
		<tr>
			<td>3.2</td>
			<td>1.50.11</td>
			<td>150</td>
		</tr>
		<tr>
			<td>3.3</td>
			<td>3.30.6</td>
			<td>330</td>
		</tr>
		<tr>
			<td>4.0</td>
			<td>4.00.9</td>
			<td>400</td>
		</tr>
		<tr>
			<td>4.1</td>
			<td>4.10.6</td>
			<td>410</td>
		</tr>
		<tr>
			<td>4.2</td>
			<td>4.20.11</td>
			<td>420</td>
		</tr>
		<tr>
			<td>4.3</td>
			<td>4.30.8</td>
			<td>430</td>
		</tr>
		<tr>
			<td>4.4</td>
			<td>4.40.9</td>
			<td>430</td>
		</tr>
		<tr>
			<td>4.5</td>
			<td>4.50.7</td>
			<td>450</td>
		</tr>
		<tr>
			<td>4.6</td>
			<td>4.60.5</td>
			<td>460</td>
		</tr>
	</table>

	<h3>GLSL Operators</h3>

	<p>
		GLSL contains the operators in C and C++, with the exception of pointers. Bit-wise
		operators were added in version 1.30.
	</p>

	<p>
		If you leave out the version tag, OpenGL falls back to an earlier default - it's
		always better to specify the version.
	</p>

	<h3>GLSL Data Types</h3>

	<p>
		The most commonly used data types in GLSL are in the table below. For a complete list
		see any of the <a href="http://www.opengl.org/documentation/glsl/">official reference documents</a>.
	</p>

	<table>
		<caption><b>Commonly-Used GLSL Data Types</b></caption>
		<tr>
			<th>Data Type</th>
			<th>Description</th>
			<th>Common Usage</th>
		</tr>
		<tr>
			<td><tt>void</tt></td>
			<td>nothing</td>
			<td>Functions that do not return a value</td>
		</tr>
		<tr>
			<td><tt>bool</tt></td>
			<td>Boolean value as in C++</td>
		</tr>
		<tr>
			<td><tt>int</tt></td>
			<td>Signed integer as in C</td>
		</tr>
		<tr>
			<td><tt>float</tt></td>
			<td>Floating-point scalar value as in C</td>
		</tr>
		<tr>
			<td><tt>vec3</tt></td>
			<td>3D floating-point value</td>
			<td>Points and direction vectors</td>
		</tr>
		<tr>
			<td><tt>vec4</tt></td>
			<td>4D floating-point value</td>
			<td>Points and direction vectors</td>
		</tr>
		<tr>
			<td><tt>mat3</tt></td>
			<td>3x3 floating-point matrix</td>
			<td>Transforming surface normals</td>
		</tr>
		<tr>
			<td><tt>mat4</tt></td>
			<td>4x4 floating-point matrix</td>
			<td>Transforming vertex positions</td>
		</tr>
		<tr>
			<td><tt>sampler2D</tt></td>
			<td>2D texture loaded from an image file</td>
		</tr>
		<tr>
			<td><tt>samplerCube</tt></td>
			<td>6-sided sky-box texture</td>
		</tr>
		<tr>
			<td><tt>sampler2DShadow</tt></td>
			<td>shadow projected onto a texture</td>
		</tr>
	</table>

	<h3>File Naming Convention</h3>

	<p>
		Each shader is written in plain text and stored as a character array (C string).
		It is usually convenient to read each shader from a separate plain text file. I
		use a file naming convention like this;
	</p>

	<table>
		<caption><b>A GLSL File Naming Convention</b></caption>
		<tr>
			<td><tt>font.vert</tt></td>
			<td>The vertex shader for my text rendering shader program.</td>
		</tr>
		<tr>
			<td><tt>font.frag</tt></td>
			<td>The fragment shader for my text rendering shader program.</td>
		</tr>
		<tr>
			<td><tt>particle.vert</tt></td>
			<td>The vertex shader for a particle system shader.</td>
		</tr>
		<tr>
			<td><tt>particle.geom</tt></td>
			<td>The geometry shader for a particle system shader.</td>
		</tr>
		<tr>
			<td><tt>particle.frag</tt></td>
			<td>The fragment shader for a particle system shader.</td>
		</tr>
	</table>

	<p>
		Some coding text editors will do syntax highlighting for GLSL
		if you end with a ".glsl" extension. The GLSL reference compiler;
		<a href="http://www.khronos.org/opengles/sdk/tools/Reference-Compiler/">Glslang</a>
		can check your shaders for bugs if they end in ".vert" and ".frag".
	</p>

	<h2>Expanding the Vertex Shader with a Uniform</h2>

	<p>
		GLSL is designed to resemble the C programming language. Each shader resembles a small
		C program. In <a href="hellotriangle.html">Hello Triangle</a> we wrote a very minimal shader program that has only a
		vertex
		shader and a fragment shader. Each of these shaders can be stored in a C string, or in
		a plain text file first, for convenience, and then loaded into a string. Let's start from a working Hello Triangle
		program,
		and modify it to add some new features.
	</p>

	<p>
		Recall that our vertex shader has the job of positioning any one of the vertex points, from a vertex buffer,
		inside the view area. It does this by assigning a value to the built-in GLSL variable <tt>gl_Position</tt>.
		The viewing area is actually a 3D volume with dimensions -1 to 1 on the x, y, and z axes.
		This volume is called <i>clip space</i>, as any parts of geometry outside it will be "clipped" off, and not
		rendered.
	</p>

	<p>
		We can make our vertex shader a little bit more fun by modifying our triangle's position. If we use the current time
		as a variable, we can animate this too. Let's modify our vertex shader, and add in a <tt>uniform</tt> for the
		current time.
		I wont add string quotes or line endings here. If you're not loading the shader from a file, you'll need to add
		that.
	</p>

	<pre>
#version 410 core

in vec3 vertex_position;
uniform float time;      // uniform is a keyword in GLSL

void main() {
  vec3 pos = vertex_position;
  pos.y += sin( time );

  gl_Position = vec4( pos, 1.0 );
}</pre>

	<p>
		The input to a vertex buffer (the <tt>in</tt> variables) are called per-vertex
		<i>attributes</i>, and come from blocks of memory on
		the graphics hardware memory called <i>vertex buffers</i>. We usually copy our vertex
		positions into vertex buffers in our C program, before running our main loop. We will look at vertex
		buffers in the next tutorial. This vertex shader will run one instance for every vertex
		in the vertex buffer.
	</p>

	<p>
		You'll notice the new line <tt>uniform float time;</tt> declares a new variable.
		Instead of <tt>in</tt> for attribute, we have <tt>uniform</tt> <i>storage qualifier</i> here. A
		storage qualifier basically says where the data comes from or goes to. In this case it will come from a variable
		in our C program that we'll feed into OpenGL.
		The data type here is a <tt>float</tt>, as we also know from C. I've named it <tt>time</tt>.
		There is no rule here, call it whatever you want, as long as it's not the same as a GLSL keyword.
	</p>

	<div class="hint">
		Uniforms in GLSL are actually constants in our shaders, meaning we can read, but not change the values.
		In the Microsoft Direct3D API they are named <i>constants</i>.
	</div>

	<p>
		I've also created a new <tt>vec3</tt> variable in our shader called <tt>pos</tt>. I'm just making it a copy of the
		vertex position
		value from our vertex buffer to begin with.
		You can access individual components of a complex type like a <tt>vec3</tt>.
		A <tt>vec4</tt> has <tt>.x .y .z .w</tt>, or, alternatively, because these types can also represent colours, <tt>.r
			.g .b .a</tt>.
		On the line <tt>pos.y += sin( time );</tt> I'm modifying only the vertical component of the position. Note the
		<tt>+=</tt>
		operator. I'm using the built-in <tt>sin()</tt> (sine wave) function to return a value between -1.0 and 1.0. That's
		handy because it won't move
		the triangle far outside the viewable area. I'm using the time value, which will be the current time, in seconds as
		the <tt>x</tt> value.
		In the last instruction note that I'm using our new <tt>pos</tt> variable for the final vertex position in clip
		space.
	</p>

	<p>
		If you compile and run your program at this point it should still work without errors, and nothing should have
		changed. Why? Uniform variables default to the
		value 0, and we are adding 0 to our triangle's y position. We need to modify our C program and update the uniform
		value. If you have not done so already,
		at the top of your <tt>while</tt> loop add the following GLFW function call to get the current time every frame:
	</p>

	<pre>
double curr_s = glfwGetTime(); // Get the current time. </pre>

	<p>When our shader program is linked, every uniform value will be assigned a <i>location</i> number. We can use this
		to refer to it.
		You can fetch the location of the <tt>time</tt> uniform after the shader is linked. Call
		<tt>glGetUniformLocation()</tt>, using the the shader program's
		OpenGL handle, and the name you gave the variable. If the uniform is not found by name, then the value of -1 is
		returned. This can happen if the name
		has a typo in it, or if the variable wasn't used and the compiler optimised it out.
	</p>

	<pre>
int time_loc = glGetUniformLocation( shader_program, "time" );
assert( time_loc > -1 ); // NB. include assert.h for assert().</pre>

	<p>
		Now, right before our call to draw the triangle, we can update the uniform to our shader, using our current time,
		and the location we retrieved earlier.
		The drawing section of our <tt>while</tt> loop now looks like this, with the addition of a function call to
		<tt>glUniform1f</tt>:
	</p>

	<pre>
glClearColor( 0.6f, 0.6f, 0.8f, 1.0f );
glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT );

glUseProgram( shader_program );
glUniform1f( time_loc, (float)curr_s); // <--- new line
glBindVertexArray( vao );

glDrawArrays( GL_TRIANGLES, 0, 3 );</pre>

	<p>
		The <tt>glUniform</tt> family of functions have variations matching all the GLSL data types. Here, I'm updating a
		GLSL <tt>float</tt> so I call
		<tt>glUniform1f()</tt> for 1 (f)loat. If I was updating a <tt>vec3</tt> uniform, then I'd call
		<tt>glUniform3f()</tt>. Look these functions up on
		<a href="https://docs.gl/gl4/glUniform">docs.gl</a> to see the full list, and how the parameters differ. Note that I
		explicitly cast our time
		value to a <tt>float</tt>, as it's a <tt>double</tt> normally. <tt>glUniform</tt> calls operate only on the
		<i>currently bound</i> shader program,
		so I called our new function <i>after</i> the call to <tt>glUseProgram()</tt>. It won't matter in our 1-shader
		program, but it will if you add in another
		shader program later. If you are using OpenGL 4.1, or newer, then you can use an alternative function
		family, <a href="https://docs.gl/gl4/glProgramUniform">glProgramUniform</a>, which does not have this limitation.
	</p>

	<p>
		Compile, and run your program - you should get a bouncing triangle. You might have set a different colour.
	</p>

	<img src="images/bouncing_triangle.gif" alt="A vertically bouncing triangle" />

	<p>
		Our vertex shader has some built-in data types that we can see here:
	</p>
	<ul>
		<li><tt>vec3</tt> is a 3D vector that can be used to store positions, directions, or colours.</li>
		<li><tt>vec4</tt> is the same but has a fourth component which, in the <tt>gl_Position</tt> variable, is used
			to represent perspective. When building a virtual camera, we can set this, <tt>w</tt>, component to produce
			perspective.
			After the vertex shader completes, there is a built-in stage called <i>perspective division</i> that divides
			the <tt>.xyz</tt> components by the <tt>.w</tt> component. We don't have anything with depth in our scene,
			so it won't be obvious what this does yet, but you can try changing the value to 0.5 and 2.0 to get an idea.
			1.0 has no effect, of course, because it is dividing the <tt>.xyz</tt> components by 1.0.
	</ul>

	<p>
		We can also see the <tt>in</tt> key-word for input to the program from the
		previous stage. In this case the <tt>vertex_position</tt> is one of the vertex
		points from the vertex buffer that we are drawing. GLSL also has an <tt>out</tt> key-word for
		sending a variable to the next shader stage in the pipeline.
	</p>

	<div class="hint">
		In larger shaders I like to be reminded what I'm working with.
		You can't change the value of uniforms or attributes, for example.
		I haven't here, but I usually use prefixes for variable storage qualifiers;
		<tt>a_</tt> for vertex shader input attributes,
		<tt>v_</tt> for <i>varyings</i> - variables passed between shader stages,
		<tt>u_</tt> for uniforms,
		and <tt>o_</tt> for fragment shader outputs like the colour of the fragment.
		If it's a variable I declared in the shader then I don't use a prefix.
	</div>

	<h3>Expanding the Fragment Shader</h3>

	<p>
		The hardware pipeline knows that the first <tt>vec4</tt> it gets as output from the
		fragment shader should be the colour of the fragment. The colours are <b>RGBA</b>, or
		red, green, blue, alpha, in that order. The values of each component are floats between 0.0 and 1.0,
		not bytes between 0 and 255, as is common elsewhere. The alpha channel output can be used for a variety of effects,
		which you define by setting a <i>blend mode</i> in OpenGL. It is commonly used to
		indicate opacity (for a transparent, or colour-blending effect), but by default it does nothing.
	</p>

	<div class="hint">
		If you're wondering why we don't set a built-in variable for output colour here, that's a good question.
		Older versions of GLSL had a built-in <tt>gl_FragColor</tt> variable to set, similar to <tt>gl_Position</tt>
		in vertex shaders. When we get into further tutorials you will see that there is a lot more flexibility
		for multiple outputs possible now.
	</div>

	<pre>
#version 410 core

uniform float time;
out vec4 frag_colour;

void main() {
  float wild = sin( time ) * 0.5 + 0.5;
  frag_colour.rba = vec3( 1.0 );
  frag_colour.g = wild;
}</pre>


	<p>
		The <tt>uniform</tt> variables are global to all shaders within the
		program, so we have accessed our <tt>time</tt> variable here too. I've created a new variable, using the
		<tt>time</tt>,
		called <tt>wild</tt>. This is similar to the position change, except we will use it for a colour component. Because
		sine
		wave values are between -1.0 and 1.0 I scale and offset it to be between a colour range of 0.0 and 1.0.
		The line <tt>frag_colour.rba = vec3( 1.0 );</tt> is just showing a different way to set variables by their
		components.
		GLSL has a mix-up ability it calls <i>swizzling</i> that lets you set any subset of components, in any order.
		Because I'm only changing 3 of the 4 components, I'm assigning a <tt>vec3()</tt>. Giving a single parameter to a
		complex type constructor sets <i>all</i> of that components to that value. Here it means that the red, blue, and,
		alpha components will be set to 1.0. You should get an interesting colour animation now. Try setting the other
		components
		to different <tt>sin()</tt> functions, perhaps one with <tt>time * 0.5</tt> as input.
	</p>


	<h3>Passing a Variable from Vertex to Fragment Shader</h3>

	<p>
		We can also output a variable from a shader to send it to the next shader stage down the pipeline. Let's modify our
		shaders to do that.
		We will specify an <tt>out</tt> variable in the vertex shader, that matches the name of an <tt>in</tt> variable in
		the fragment shader.
		The vertex shader:
	</p>

	<pre>
#version 410 core

in vec3 vertex_position;
uniform float time;
out vec3 pos;            // <-- pos now declared here

void main() {
	pos = vertex_position; // <-- removed declaration
	pos.y += sin( time );

	gl_Position = vec4( pos, 1.0 );
}</pre>

	<p>
		Now we can input that position in the fragment shader. Let's use it as a colour, commenting out our time-based code.
	</p>

	<pre>
#version 410 core

in vec3 pos;         // <-- matching name required.
uniform float time;
out vec4 frag_colour;

void main() {
	// float wild = sin( time ) * 0.5 + 0.5;
	// frag_colour.rba = vec3( 1.0 );
	// frag_colour.g = wild;

	frag_colour = vec4( pos, 1.0 );
}</pre>

	<img src="images/varying_triangle.gif" alt="A triangle using its clip space coordinates as colours" />

	<p>
		Now we are visualising the clip space coordinates of each fragment as a colour. <tt>x</tt> corresponding
		to red, <tt>y</tt> to green, and <tt>z</tt> to blue. We don't see any blue on the triangle because, if you recall,
		we set Z to 0 for all 3 vertex points. You'll note that we only set 3 positions, one for each vertex, but we get a
		whole gradient of colours over a larger number of fragments. When variables are output from a vertex to a fragment
		they are <i>interpolated</i> to each fragment, based on a fragment's distance to each vertex.
	</p>

	<div class="hint">
		These input and output variables were called <i>varyings</i> in older versions of GLSL, and used the
		<tt>varying</tt> storage qualifier
		instead of <tt>in</tt> and <tt>out</tt>.
	</div>

	<p>
		Having an input for time lets you create animations. Having a position value lets you create patterns that vary
		across
		a surface. Combining these two, and using some of the built-in functions like <tt>sin()</tt> creatively lets you
		create
		some very cool effects. If you add some sort of user input, so as the mouse cursor position, or a uniform to
		indicate if a button
		is held down, you can also make them interactive.
	</p>

	<div class="hint">A handy <a href="https://www.khronos.org/opengl/wiki/Built-in_Variable_(GLSL)">built-in variable</a> is available in fragment shaders that you can access to create
		more interesting effects. <tt>vec4 gl_FragCoord</tt> gives window-relative coordinate of the current fragment. Note that this isn't in clip coordinate space as our vertex position was,
		but in pixel dimensions. If you want the 0.0 to 1.0 range, divide the <tt>x</tt> and <tt>y</tt> components by your view resolution (we set something like 800 and 600, respectively, in Hello Triangle).
		Try using this as the output colour to get an idea.
	</div>

	<h2>Possible Extensions</h2>

	<p>
		<a href="https://antongerdelan.net/opengl/shader_hot_reload.html">Hot reloading</a> will add an incredible amount
		of experimentation power to your shader development.
		I would recommend adding this to every project where you are doing something creative with shaders. If you add a
		few interesting uniform variables
		e.g. mouse cursor position, viewport dimensions, the time, then you can have a lot of fun with some basic
		mathematics and some use of <tt>sin()</tt>
		waves.
	</p>

	<p>For some idea for the potential see the <a href="https://www.shadertoy.com/">Shadertoy</a> website, which
		lets you write and share a fragment shader
		written in GLSL using WebGL. If you add hot reloading, the view resolution, and mouse coordinates as uniforms, and make your triangle cover the screen
		(either with two triangles, or with one huge triangle where the edges are outside clip space), then you can recreate
		the basics of Shadertoy in OpenGL.
	</p>

	<p>
		If you have all of that, then you can actually take a diversion and write a <i>ray tracer</i>, or <i>path tracer</i>
		at this point.
		Each fragment would represent one ray. You would write your ray-casting code into your fragment shader. We did this
		at BTH in Sweden
		in the computer graphics class as a first assignment. If you want to check out Ray Tracing I suggest Peter Shirley's
		<a href="https://raytracing.github.io/">Ray Tracing in One Weekend</a>
		series. It's a similar style to my OpenGL e-book, and I enjoyed running through the first book. It doesn't use
		OpenGL, but you could easily adapt it.
	</p>

</body>

</html>
